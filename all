COURSE: DEVOPS WITH AWS (LINUX = FREE)

WHY  DEVOPS?
TO DELIVER THE APPLICATION/PRODUCT VERY SPEEDILY.


DEPLOYMENT: PROCESS OF INSTALLING APP FROM LOCAL MACHINE TO SERVER.


SDLC: SOFTWARE DEVELOPMENT LIFE-CYCLE

HUMAN LIFE-CYCLE: TODDLER, CHILD, TEEN, ADULT, OLD


DEV                  OPS
PLAN		     DEPLOY
CODE 		     OPERATE
BUILD		     MONITOR
TEST


COURSE: ADVANCED DEVOPS

AWS, LINUX  (35 days free)

DURATION: 3 MONTHS
FEE: 10K (2 INSTALL) [REC= 6K]
TOOLS: 24
AWS: 15+
PROJECTS: 6
EXP: 3+
TIMINGS: 9:00 - 10:30 (70 MINS CLASS 20 MINS DOUBTS)
DAYS: MON-SAT

===========================================================

ARCHITECTUTRES: 
BLUE PRINT OF OUR APPLICATION.


TYPES:
1. ONE-TIER ARCHITECTUTRE
2. TWO-TIER ARCHITECTUTRE
3. THREE-TIER ARCHITECTUTRE
4. N-TIER ARCHITECTUTRE

TIER = LAYER = SERVER

SERVER = SERVERS THE SERVICES TO END USER.
APPLICATION = COLLECTION OF SERVICES.
PAYTM: MOVIES, TRAIN, DTH ----------------


TYPES OF SERVERS:
1. WEB SERVER  = PRESENTATION LAYER
2. APP SERVER  = BUSINESS/LOGIC LAYER
3. DB SERVER   = DATA LAYER


WEB SERVER:
PURPOSE: TO SHOW THE APP
LAYER: TOP
AKA: PRESENTATION LAYER
WHO: FRONT-END/UI-UX DEV
WHAT: WEB-TECHNOLOGIES
EX: HTML, CSS, JS ----

APP SERVER:
PURPOSE: TO USE THE APP
LAYER: MIDDLE
AKA: BUSINESS/LOGIC LAYER
WHO: BACKEND DEV
WHAT: PROGRAMMING LANGUAGES
EX: JAVA, PYTHON, C, C++ ------------

DB SERVER:
PURPOSE: TO STORE & RETRIEVE DATA 
LAYER: LAST
AKA: DATA LAYER
WHO: DBA/DBD
WHAT: DB LANGUAGES
EX: MYSQL, SQL, ORACLE, POSTGRES ------------


1. ONE-TIER ARCHITECTUTRE: STAND-ALONE APP

ALL 3 SERVER WILL BE ON MY LOCAL MACHINE.
IT WILL NOT REQUIRED INTERNET CONNECTION.

EX: VLC

DIS-ADVANTAGES:
1. THE APPLICATION WILL WORK ON THAT LAPTOP ONLY.
2. WE NEED TO DOWNLOAD APP.


2. TWO-TIER ARCHITECTUTRE: CLIENT-SERVER APP

WEB, APP WILL BE ON LOCAL, DB WILL BE ON INTERNET.
IT WILL REQUIRED INTERNET CONNECTION.

EX: BANK APP

DIS-ADVANTAGES:
1. THE APPLICATION WILL WORK ON THAT LAPTOP ONLY.
2. WE NEED TO DOWNLOAD APP.


3. THEE-TIER ARCHITECTUTRE: WEB APP

ALL 3 SERVER WILL BE ON INTERNET.

ADVANATGES:
1. APP NEED NOT INSTALL
2. ANY APP FROM ANYWHERE

99% -- > WEB APP
--------------------------------------------------------------------------

SERVER: SERVERS THE SERVICES TO END USER(CLIENT).

TYPES:
1. PHYSICAL SERVER: BARE METAL OR ON-PREM 
2. VIRTUAL SERVER: CLOUD SERVER OR VIRTUAL MACHINE

LIMITATIONS:
1. SPACE 
2. MAINATINANCE
3. 24/7 IT TEAM


CLOUD PROVIDER: AWS, AZURE, GCP, ALIBABA, DIGITAL OCEAN -----------
TO CREATE A SERVER WE NEED TO HAVE A CLOUD ACCOUNT.

AWS CLOUD -- > SERVER -- > EC2 INSTANCE
EC2: ELASTIC  COMPUTE CLOUD


STEPS: 7 

1. NAME & TAGS : TO IDENITFY THE SERVER
2. AMI: AMAZON MACHINE IMAGE: OS & SOFTWARES
3. INSTANCE TYPE: HARDWARE (CPU & RAM) -- > T2.MICRO= 1 CPU & 1 GB
4. KEY PAIR: TO LOGIN -- > (1. PUBLIC: AWS 2. PRIVATE: USER)
5. NETWORK: VPC, SECURITY GROUPS

VPC: VIRTUAL PRIVATE CLOUD: TO CREATE OWN NETWORK
SG: TO ALLOW THE TRAFFIC (REQUEST & RESPONSE)
PORTS: 0 - 65535
SSH: SECURE SHELL: 22 -- > TO COMMUNICATE WITH SERVER

6. STORAGE: EBS VOLUMES: ELASTIC BLOCK STORAGE
MIN: 8 GB MAX: 16TB

7. SUMMARY:


=======================================================================================================================


LINUX: IT IS AN OS -- > BLENDER
IT IS A KERNEL -- > CORRECT

KERNEL: LOWEST LEVEL OF OS.

OS: ITS A SOFTWARE USED TO COMMUINACATE BLW USER & SYSTEM.

TYPES:
1. WINDOWS
2. MAC-OS


WHERE IS LINUX:
96% SERVERS
86% SMART PHONE
ELECTRONIC GADGETS
LAPTOPS 


USED ON:
VFX INDUSTRY
SPACE STATIONS
OIL & REFINERY INDUSTRY
STOCK MARKET
SATELLITES
MILITARY
CRICKET

WHAT IS LINUX:
IT IS A KERNEL.
ITS FREE & OPEN SOURCE (SEE, CHANGE & DISTRIBUTE THE CODE)
IT IS WRITTEN ON C LANGUAGE.
WHO: LINUS TORVALDS
YEAR: 1991

FLAVORS/DISTRIBUTION:

IPHONE: 15 -- > 15 PLUS -- > 15 PRO  -- > 15 PRO MAX

LINUX:
UBUNTU : 1
CENTOS : 2
REDHAT : 3
FEDORA
OPEN SUSE
KALI LINUX
ROCKY LINUX
ALMA LINUX
AMAZON LINUX
DEBIAN

ARCHITECTURE:

1. SHELL   : it will take input (command, script, program)
2. DAEMON  : background services (booting process)
3. KERNEL  : interact with hardware (cpu, ram & rom)

commands:
sudo -i		: to switch to root user
date		: to show the date
uptime		: shows the server run time
ip addr
ip addr show
ifconfig
hostname -i	: to show ip address
clear (ctrl l)	: to clear the screen
hostnamectl set-hostname raham : to set our hostname

===================================================================================

FILE COMMANDS:

touch file1	: to create a file
ls/ll		: to list the files
cat file1	: to print the content of a file
cat>>file1	: to insert the content
enter ctrl d	: to save the content
cp file1 file2	: to copy content from file1 to file2
mv file1 abc	: to rename a file
rm file1	: to delete file1
rm file -f	: to delete file1 forcefully
rm * -f		: to delete all files forcefully


head file1	: to print top 10 lines
head -5 file1	: to print top 5 lines
head -12 file1	: to print top 12 lines
tail file1	: to print bottom 10 lines
tail -5 file1	: to print bottom 5 lines
tail -12 file1	: to print bottom 12 lines
sed -n '5,15p' file1: to print from line 5 to 15
wc file1	: to print lines, words, characters of file1
cat -n file1	: to print line numbers along with content

------------------------------------------------------------------------

GREP: GLOBAL REGULAR EXPRESSION PRINT
PURPOSE: TO FIND WORDS


grep word1 file1	: to search for word1 in file1
grep word1 file1 -i	: -i: avoid case sensitive
grep word1 file1 -c	: -c: count
grep word1 file1 -v	: -v: ignore the line completely
grep 'word1\|word2' file1: to search for multiple words

===================================================================

EDITORS: to insert, edit, modify the content in a file

TYPES:
1. VI/VIM
2. NANO

i	: insert mode -- > to insert the content
esc	: to exit from insert mode


1. command mode
2. insert mode
3. save mode

SAVE MODE:
:w	: to save
:q	: to quit
:wq	: to save & quit

INSERT MODE:
i	: to insert the content
I	: starting of line
A	: ending of line
O	: create new line above the existing
o	: create new line below the existing

COMMAND MODE:
gg	: top of file
shift g	: bottom of file
7gg	: go to 7 th line
:7	: go to 7 th line
yy	: copies single line
3yy	: copies three lines
p	: paste one time
3p	: pastes three time
dd	: delete single line
3dd	: delete three lines
u	: undo
/word	: to search inside a file
:set number: to print line numbers


SED: stream editor
purpose: to find and replace a words/lines


sed -i 's/word1/word2/g' file1	: to replace word1 with word2
sed -i 's/word1/word2/g; s/word3/word4/g' file1	: to replace multiple words
sed -i '1c hello everyone' file1  : to replace line1
sed -i '2d' file1	: deletes line 2 in file1
sed -i '2a hello' file1	: add content after line 2 in file1


LOAD: htop

=============================================================

GIT: GLOBAL INFORMATION TRACKER
VCS: VERSION CONTROL SYSTEM
SCM: SOURCE CODE MANAGEMENT


VCS: TO STORE EACH VERSION OF CODE SEPERATELY.

V1 : 1 SERVICES : 100 LINES -- > REPO-1
V2 : 2 SERVICES : 200 LINES -- > REPO-2
V3 : 3 SERVICES : 300 LINES -- > REPO-3

WHY WE NEED TO STORE THEM SEPERATELY >
TO DO ROLLBACK: GOING BACK TO PREVIOUS VERSION OF APPLICATION
V3  -- > V2 

V1 : INDEX.HTML : 10
V2 : INDEX.HTML : 20
V3 : INDEX.HTML : 30



WHAT IS GIT?

Git is used to track the files.
It will maintain multiple versions of the same file.
It is platform-independent.
It is free and open-source.
They can handle larger projects efficiently.
It is 3rd generation of vcs.
it is written on c programming
it came on the year 2005

CVCS: CENTRALIZED VERSION CONTROL SYSTEM
STORES CODE ON SINGLE REPO
Ex: SVN

DVCS: DISTRIBUTED VERSION CONTROL SYSTEM
STORES CODE ON MULTIPLE REPO
Ex: GIT


STAGES/ARCHITECTURE OF GIT:

IN GIT WE HAVE TOTAL 3 STAGES:

1. WORKING DIRECTORY	: WHERE WE WRITE THE CODE
2. STAGING AREA		: WHERE WE TRACK THE CODE
3. REPOSITORY		: WHERE WE STORE THE TRACKED THE CODE



PRACTICAL PART:
CREATE AN EC2 INSTANCE AND CONNECT TO BROWSER

yum install git -y : to install git

yum: package manager
install: action
git: package name 
-y : yes 

git --version
git -v


mkdir Paytm
cd Paytm

git init : used to Initialize the empty repository
with out .git commands will not work.


touch index.html	: to create a file
git status		: to show the file status 
git add index.html	: to track the file
git commit -m "commit-1" index.html	: to commit a file
git log			: to show commits history
git log --oneline	: commits on single line
git log --oneline -2	: to show last 2 commits


create -- > track -- > commit
touch  -- > git add -- > git commit


git show: to show the file that committed for specific commit id.
git show commit_id

==================================================================
USER CONFIGURATION:
git config user.name "raham"
git config user.email "raham@gmail.com"


BRANCHES:
It's an individual line of development for code.
we create different branches in real-time.
each developer will work on their own branch.
At the end we will combine all branches together.
Default branch is Master.

git branch		: to list the branches
git branch movies	: to create a new branch
git checkout movies	: to switch from one branch to another.
git checkout -b recharge: to create and switch from one branch to another.
git branch -m old new	: to rename a branch
git branch -D movies	: to delete a branch

we cant delete the current branch

PROCESS:

git branch		
git branch movies	
git checkout movies
touch movies{1..5}
git status
git add movies*
git commit -m "dev-1 commits" movies*
git branch dth	
git checkout dth
touch dth{1..5}
git status
git add dth*
git commit -m "dev-2 commits" dth*
git checkout -b train
touch train{1..5}
git add train*
git commit -m "dev-3 commits" train*
git checkout -b recharge
touch recharge{1..5}
git add recharge*
git commit -m "dev-4 commits" recharge*


Note: here every dev works on the local laptop
at the end we want all dev codes to create an application.
so here we use GitHub to combine all dev codes together.

Create a GitHub account and create Repo 


git remote add origin https://github.com/revathisiriki78/paytm.git
git push origin movies
username:
password:

Note: in github passwords are not accepted we need to use token 

profile -- > settings -- > developer settings -- > Personal access token -- > classic -- > 
generate new token -- > classic -- > name: paytm -- > select 6 scopes -- > generate 

git push origin dth
username:
password:


git push origin train
username:
password:

git push origin recharge
username:
password:

=================================================================

GIT CLONE: it download code from github(Remote) to git(Local).
git clone https://github.com/anitalluri00/paytm.git


GIT FORK: it download code from github account to another.
For git clone and git fork repos must be public.

PUBLIC REPO: source code will be visible from internet.
PRIVATE REPO: source code will be hidden from internet.


GIT MERGE: it will merge files blw two different branches

git checkout master
git merge movies
git merge train


GIT REBASE: used to add files blw two different branches
git checkout master
git rebase train
git rebase recharge


MERGE VS REBASE:
merge for public repos, rebase for private 
merge stores history, rebase will not store the entire history
merge will show files, rebase will not show files


in GitHub we use Pull Request (PR) to do merging.


GIT REVERT: used to revert(get back) the files we merge.
to undo merge we can use revert.

git revert dth
git revert recharge

====================================================================

GIT RESTORE:
this command will untrack the tracked file.
git rm --cached file_name 		: before inital commit
git restore --staged file_name		: after inital commit

this command used to recover the deleted file.
git restore file_name

GIT PULL:
used to get the changed files from github to git.
git pull origin master

GIT FETCH:
used to show the changed files from github to git.
git fetch

CHERRY-PICK: Merging the specific files based on commits.
git cherry-pick commit_id

Process:

mkdir raham
cd raham/
touch java{1..3}
git init
git add java*
git commit -m "java-commits" java*
git branch
git checkout -b branch1
touch python{1..5}
git add python*
git commit -m "python commits" python*
touch php{1..5}
git add php*
git commit -m "php commits" php*
touch donet{1..5}
git add donet*
git commit -m "dontent commits" donet*
git log --oneline
git checkout master
ll
git cherry-pick commit_id_of python
ll
git cherry-pick commit_id_of php


MERGE CONFLICTS:
it will rise when we merge 2 different branches with same files.
How to resolve: Manually 


vim file1
im dev-1 writing java on branch-1
git add index.html
git commit -m "dev-1 commits" index.html
git branch -m master branch1

git checkout -b branch2
vim file1
im dev-2 writing index.html on branch-2
git add index.html
git commit -m "dev-2 commits" index.html

git checkout branch1
vim file1
im dev-1 writing index.html on branch-1
new line
git add index.html
git commit -m "dev-1 2nd commits" index.html
git merge branch2


vim index.html
git add index.html
git commit -m "merge commits"


.gitignore: this file will ignore the files of being tracked.
if you write any filename on this .gitingore it wont track that file.
USECASE: cred

Note: . should be mandatory


GIT RESET: to undo the commits.

LONG LIVING BRANCHES: these branches we wont delete and we use them frequently.
EX: Master, Main

SHORT LIVING BRANCHES: these branches we will delete and we dont use them frequently.
EX: release, bugfix, --------

==================================================================================

31-07-2024


MAVEN:

raw chicken -- >  clean  --> marnet -- > ingredients -- > Chicken Biryani
raw code    -- > build   -- > test  -- > artifact -- > Deployment

ARTIFACT: its final product of our code.
developers will give raw code that code we are going to convert into artifact.

TYPES:
1. jar	: Java Archive       : Backend code
2. war	: Web Archive	     : Frontend code + Backend code
3. ear	: Enterprise Archive : jar + war 

JAR FILE:

.java -- > compile -- > .class -- > .jar file

.java	: basic raw
.class	: executable file
.jar	: artifact

all the artifacts are going to created by a build too1.

MAVEN:
Maven is the build tool.
its a free and opensource.
build: process of adding the libs & dependencies to code.
its is also called as Project management too1.
it will manage the complete structure of the project.
the main file in the maven tool is POM.XML

POM.XML: its a file which consist of complete project information.
Ex: name, artifact, tools, libs, dep --------

POM: PROJECT OBJECT MODEL
XML: EXTENSIBLE MARKUP LANGUAGE

WHO GIVE POM.XML : DEVELOPERS
dev will give both code and pom.xml in github

maven is written on java by apache software foundation.
supports: JAVA-1.8.0
year: 2004
home path: .m2


PRATCICAL PART:
1. CREATE AN EC2 INSTANCE AND CONNECT
2. yum install git java-1.8.0-openjdk maven tree -y
3. git clone https://github.com/devopsbyraham/jenkins-java-project.git
4. cd jenkins-java-project


MAVEN LIFECYCLE:
GOALS : a command used to run a task.
Goals refers pom.xml to execute.


PLUGIN: its a small software with makes our work automated.
insted of downloading tools we can donwload plugins.
this plugins will donwload automatically when we run goals.



1. mvn compile : used to compile the code (.java [src] -- > .class [target])
2. mvn test    : used to test the code    (.java [test] -- > .class [target])
3. mvn package : used to create artifact 
4. mvn install : used to copy artifact to .m2 (project folder -- > .m2)
5. mvn clean   : to delete the target folder
6. mvn clean package: compile -- > install


PROBLEMS WITHOUT MAVEN:
1. we cant create artifacts.
2. We cant create project structure.
3. we cant build and deploy the apps.

ALTERNATIVIES:
MAVEN, ANT, GRADLE 

MAVEN VS ANT:
1. MAVEN IS BUILD & PROJECT MANAGEMNT, ANT IS ONLY BUILD TOOL
2. MAVEN HAS POM.XML, ANT HAS BUILD.XML
3. MAVEN HAS A LIFECYCLE, ANT WILL NOT HAVE LIFECYCLE
4. MAVEN PLUGINS ARE REUSABLE, ANT SCRIPTS ARE NOT RESUEABLE.
5. MAVEN IS DECLARATIVE, ANT IS PROCEDURAL.

PROGRAMMING VS BUILD:

JAVA	: MAVEN
PYTHON	: GRADLE
.NET	: VS CODE
C, C#	: MAKE FILE
node.js	: npm

ALTERNETIAVES: 
ANT, GRADLE for java projects.



======================================================================================

01-08-2024:

JENINS IS A CI/CD TOOL.
REALITY: JENKINS IS ONLY FOR CI.

CI : CONTINOUS INTEGRATION : CONTINOUS BUILD + CONTINOUS TEST (OLD CODE WITH NEW CODE)

DAY-1: 100 LINES : BUILD + TEST
DAY-2: 200 LINES : BUILD + TEST
DAY-3: 300 LINES : BUILD + TEST

BEFORE CI:
MANUAL PROCESS
TIME WASTE

AFTER CI:
AUTOMATED PROCESS
TIME SAVING

CD: CONTINOUS DELIVERY/DEPLOYMENT

ENV:

PRE-PROD/NON-PROD:
DEV	: developers
QA	: testers
UAT	: clients

LIVE/PROD ENV:
PROD	: users

CONTINOUS DELIVERY: Deploying the application to production in manual.
CONTINOUS DEPLOYMENT: Deploying the application to production in automatic.


PIPELINE: 

WAKEUP -- > DAILY ACTIVITIES -- > BREAKFAST -- > LUNCH -- > CLASS
CODE -- > COMPILE -- > TEST -- > ARTIFACT -- > DEPLOY

SETP BY STEP EXECUTION OF A PROCESS.
SERIES OF EVENTS INTERLINKED WITH EACHOTHER.


JENKINS: 
ITS A FREE AND OPEN-SOURCE TOOL.
IT IS PLATFORM INDEPENDENT.
JENKINS WRITTEN ON JAVA.
IT CONSIST OF PLUGINS.
WE HAVE COMMUNITY SUPPORT.
IT CAN AUTOMATE ENTIRE SDLC.
IT IS OWNED BY SUN MICRO SYSTEM AS HUDSON.
HUDSON IS PAID VERSION.
LATER ORACLE BROUGHT HUDSON AND MAKE IT FREE.
LATER HUDSON WAS RENAMED AS JENINS.
INVENTOR: Kohsuke Kawaguchi
PORT NUMBER: 8080
JAVA: JAVA-11/17
DEFAULT PATH: /var/lib/jenkins

ALTERNATIVES:
BAMBOO, GO CI, CIRCLE CI, TARVIS, SEMAPHORE, BUDDY BUILD MASTER, GITLAB, HARNESS
ARGOCD -----

CLOUD: AWS CODEPIPELINE, AZURE PIPLEINE ---------------------



SETUP: Create an EC2 and Include all traffic in sg

#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA11 AND JENKINS
amazon-linux-extras install java-openjdk11 -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: START JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service

CONNECT:
copy-public-ip:8080 (browser)
cat /var/lib/jenkins/secrets/initialAdminPassword (server)
paster password on browser -- > installing plugins --- > user details -- > start


JOB: it is used to perform task.
to do any work or task in jenkins we need to create a job.

to run commands we need to select execute shell on build steps.

build now: to run job
workspace: place where our job outputs will store

CREATING A JOB:
NEW ITEM -- > NAME: ci-job -- > FREESTYLE -- > OK -- > SCM -- > GIT -- > REPOURL: https://github.com/devopsbyraham/jenkins-java-project.git -- >Build Steps -- > ADD Build Steps -- > Execute shell -- > mvn clean package -- > save -- > build now

WORKSPACE: where your job output is going to be stored
Default: /var/lib/jenkins/workspace


CUSTOM WORKSPACE: storing the jobs output on our own folders
cd 
mkdir raham
cd /
chown jenkins:jenkins /root
chown jenkins:jenkins /root/raham

=======================================================================

SETTING CI SERVER USING SCRIPT:

CREATE A SERVER
sudo -i 

vim jenkins.sh

#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA11 AND JENKINS
amazon-linux-extras install java-openjdk11 -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: RESTARTING JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service

:wq

To run script: sh jenkins.sh

To execute commands on jenkins use execute shell under build steps.

VARIABLES:
it is used to store values that are going to change frequently.
ex: date, season -----



TYPES OF VARIABLES IN JENKINS:
1. USER DEFINED
2. JENKINS ENV

1. USER DEFINED VARIABLES: these are defined by user

a. Local Variable: Variable will work inside of job.
will be working for only single job.

NEW ITEM -- > NAME: ABC -- > FREESTYLE -- > OK -- > BUILD -- >EXECUTE SHELL

name=raham
echo "hai all my name is $name, $name is from hyderabad, $name is teaching devops"

WITH PARAMETERS:
take branch as variable 

passing parameters:
configure -- > this project is parameterized -- > choice -- > name: branch values: main, master --
build with parameters.

b. Global Variable: Variable will work outside of job.
will be working for multiple job.


Dashboard -- > Manage Jenkins -- > System -- > Global properties  -- > Environment variables -- > add : Name: name value: raham -- > save 

NOTE: while defining variables spaces will not be given.
local variables will be high priority.

Limitation: some values cant be defined by user because these values will change build by build.
ex: build number, time, name, url -----

2. JENKINS ENV VARIABLES: these are defined by Jenkins itself.
a. these variables can be change from build to build.
b. these variables will be on upper case.
c. these variables can be defined only once.

echo "the build number is $BUILD_NUMBER, the job name is $JOB_NAME"

printenv: gives all env vars of jenkins



find / -name jenkins.service
find command used to find the path of a file.

when we modify any file from system folder we need to reload daemon

ADMIN TASKS:
1. CHANGING PORT NUMBER OF JENKINS:

vim /usr/lib/systemd/system/jenkins.service
line-70: 8080=8090 -- > save and exit
systemctl daemon-reload
systemctl restart jenkins.service

When we change configuration of any service we need to restart.

2. PASSWORDLESS LOGIN

vim /var/lib/jenkins/config.xml
line-10: true=false
systemctl daemon-reload
systemctl restart jenkins.service

now check the jenkins dashboard it wont ask password


3. HOW TO RESOLVE THE ISSUE IF JENKINS SERVER CRASHED ?
stop the jenkins server and start it 
systemctl restart jenkins

When we stop server the services will be also stopped
so we want to restart them 

systemctl stop jenkins.service
systemctl restart Jenkins.service


BUILD EXECUTORS & PARALLEL BUILDS:
Jenkins will run the jobs sequentially (one by one)
if i want to run multiple builds at same time we can configure like this

job -- > configure -- > Execute concurrent builds if necessary -- > save -- > build now 2 times
now we can see 2 jobs will be running on same time.

BUILD EXECUTORS: max number of builds we can run
build-executor status -- > Built-In Node -- > Configure -- > 2 - 5 -- >save
now build 5 times


WHEN WE STOP SERVER:
1. IP WILL CHANGE
2. SERVICES WILL BE STOPPED


================================================================================

CRON JOB: We can schedule the jobs that need to be run at particular intervals.
here we use cron syntax
cron syntax has * * * * *
each * is separated by space

*	: minutes
*	: hours
*	: date
*	: month
*	: day of week (sun=0, mon=1 ----)

10:30 pm 4 aug sun
21 9 3 8 6
create a ci job -- > Build Triggers -- > Build periodically -- > * * * * * -- > save

CRONTAB-GENERATOR: https://crontab.guru/

limitation: it will not check the code is changed or not.


POLL SCM: 
in pollscm we will set time limit for the jobs.
if dev commit the code it will wait until the time is done.
in given time if we have any changes on code it will generate a build

create a ci job -- > Build Triggers -- > poll scm -- > * * * * * -- > save
commit the changes in GitHub then wait for 1 min.

LIMITATION:
1. in pollscm, we need to wait for the time we set.
2. we will get the last commit only.

WEBHOOK: it will trigger build the moment we change the code.
here we need not to wait for the build.

repo -- > settings -- > webhooks -- > add webhook -- > Payload URL (jenkins url) -- > http://35.180.46.134:8080/github-webhook/  -- > Content type -- > application/json -- > add

create ci job -- > Build Triggers: GitHub hook trigger for GITScm polling -- > save


BUILD SCRIPTS: to make jenkins builds from remote loc using script/
give token 
give url on other browser.

THROTTLE BUILD:

To restrict the builds in a certain time or intervals.
if we dont restrict due to immediate builds jenkins might crashdown.

by default jenkins will not do concurrent builds.
we need to enable this option in configuration.

Execute concurrent builds if necessary -- > tick it

create a ci job -- > configure -- > Throttle builds -- > Number of builds: 3 -- > time period : hours -- > save

now it will take 20 minutes of gap for each build.

CUSTOM THEMES:

dashboard -- > manage Jenkins -- > Appearance -- > Customizable theme -- > CSS
https://github.com/alefnode/jenkins-themes


================================================================
PIPELINE: STEP BY EXECUTION OF A PROCESS
SERIES OF EVENTS INTERLINKED WITH EACH OTHER.
code -- > build -- > test -- > artifact -- > deployment
IF PIPELINE FAILS ON STAGE-1 IT WONT GO FOR STAGE-2.

plugin download:
Dashboard
Manage Jenkins
Plugins
available 
pipeline stage view
install


why to use ?
to automate the work.
to have clarity about the stage.

TYPES:
1. DECLARATIVE
2. SCRIPTED

pipeline syntax:
to write the pipeline we use DSL.
We use Groovy Script for jenkins Pipeline.
it consists of blocks that include stages.
it includes () & {} braces. 


SHORTCUT: PASSS

P	: PIPELINE
A	: AGENT
S	: STAGES
S	: STAGE
S	: STEPS 


SINGLE STAGE: this pipeline will have only one stage.

EX-1:
pipeline {
    agent any 
    
    stages {
        stage('abc') {
            steps {
               sh 'touch file1'
            }
        }
    }
}

EX-2:
pipeline {
    agent any 
    
    stages {
        stage('raham') {
            steps {
                sh 'touch file2'
            }
        }
    }
}

MULTI STAGE: this pipeline will have more than one stage.

pipeline {
    agent any
    
    stages {
        stage ('two') {
            steps {
                sh 'lsblk'
            }
        }
        stage ('three') {
            steps {
                sh 'lscpu'
            }
        }
        stage ('four') {
            steps {
                sh 'lsmem'
            }
        }
    }
}


CI PIPELINE:

CODE + BUILD + TEST + ARTIFACT

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}


PIPELINE AS A CODE: Running more than one command/action inside a single stage.
to reduce the length of the code.
to save the time.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh 'mvn compile'
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

MULTI STAGE PIPELINE AS A CODE: Running more than one command/action in multiple stages.


pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh 'mvn compile'
            }
        }
        stage ('two') {
            steps {
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

PAAC OVER SINGLE SHELL: Running all the shell commands on a single shell.

pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh '''
                mvn compile
                mvn test
                mvn package
                '''
            }
        }
    }
}


INPUT PARAMETERS: BASED ON USER INPUT THE PIPELINE IS GOING TO EXECUTE.
used to avoid the mistakes.
pipeline will wait until we provide the input.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage ('deploy') {
            input {
                message "is your inputs correct ?"
                ok "yes"
            }
            steps {
                echo "my code is deployed"
            }
        }
    }
}
NOTE: In real time providing manual approval is best practise for Jenkins pipelines.

NOTE: if we have syntax issue none of the stages will execute.

DIFF BLW SCRIPTED VS DECLARATVE

SCRIPTED: 	DECLARATIVE: 
SHORT   	LONG
NO STAGES       IT HAS STAGES
START NODE      START WITH PIPELINE

================================================================

MASTER AND SLAVE:
it is used to distribute the builds.
it reduce the load on jenkins server.
communication blw master and slave is ssh.
Here we need to install agent (java-11).
slave can use any platform.
SLAVE IS your instance.
label = way of assigning work for slave.

SETUP:
#STEP-1 : Create a server and install java-11
amazon-linux-extras install java-openjdk11 -y

#STEP-2: SETUP THE SLAVE SERVER
Dashboard -- > Manage Jenkins -- > Nodes  -- > New node -- > nodename: abc -- > permanaent agent -- > save 

CONFIGURATION OF SALVE:

Number of executors : 3 #Number of Parallel builds
Remote root directory : /tmp #The place where our output is stored on slave sever.
Labels : swiggy #place the op in a particular slave
useage: last option
Launch method : last option 
Host: (your privte ip)
Credentials -- > add -- >jenkins -- > Kind : ssh username with privatekey -- > username: ec2-user 
privatekey : pemfile of server -- > save -- > 
Host Key Verification Strategy: last option

DASHBOARD -- > JOB -- > CONFIGURE -- > RESTRTICT WHERE THIS JOB RUN -- > LABEL: SLAVE1 -- > SAVE

BUILD FAILS -- > WHY -- > WE NEED TO INSTALL PACKAGES
yum install git java-1.8.0-openjdk maven -y


pipeline {
    agent {
        label 'two'
    }
    
    stages {
        stage('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh '''
                mvn compile
                mvn test
                mvn package
                mvn install
                mvn clean package
                '''
            }
        }
    }
}



POST BUILD ACTIONS:
Actions that perform after build is done.


1. always	: executes always
2. success	: executes when build is success only
3. failure	: executes when build is failed only


pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('Artifacts') {
            steps {
                sh 'mvn clean package'
            }
        }
    }
    post {
        always {
            echo "this build is completed"
        }
    }
}
==========================================================================


NEXUS:
Its an Artifactory storage service.
used to store artifacts on repo. (.JAR, .WAR, .EAR)
Nexus server -- > Repo -- > Artifact
we can use this server to rollback in real time.
it req t2.medium 
nexus uses java-1.8.0
PORT: 8081

ALTERTAVIVES: JFROG, S3, -----

SETU SCIPT:
https://github.com/RAHAMSHAIK007/all-setups.git

STEPS: signin -- > username: admin & passowrd: /app/sonatype-work/nexus3/admin.password -- > next -- > set password -- > disable anonymous access -- > save

CREATING REPO:
settings symbol -- > repositories -- > new -- > maven2(hosted) -- > name -- > save

NOTE: to integrate any tool with Jenkins we need to download the plugin.

NEXUS INTEGRATION TO PIPELINE:
1. Download the plugin (Nexus Artifact Uploader)
Manage Jenkins -- > plugins -- > Available Plugins -- > Nexus Artifact Uploader -- > install.
2. Configure it to pipeline by using pipeline syntax

NOTE: All the information will be available on pom.xml file.


PIPELINE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Artifact upload') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: '968c23dd-b648-4f15-91bf-7d76981a1218', groupId: 'in.RAHAM', nexusUrl: '100.25.197.110:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'netflix', version: '1.2.2'
            }
        }
    }
}


TOMCAT:

WEBSITE: FRONTEND -- > DB IS OPT
WEBAPP: FRONTEND + BACKEND -- > DB IS MANDATORY

WEB APPLICATION SERVER/APPLICATION SERVER/APP SERVER = TOMCAT

ITS A WEB APPLICATION SERVER USED TO DEPLOY JAVA APPLICATIONS.
IT IS WRITTEN ON JAVA LANGUAGE.
AGENT: JAVA
PORT: 8080
WE CAN DEPLOY OUR ARTIFACTS.
ITS FREE AND OPENSOURCE.
Its Platform Independent.
YEAR: 1999 

war : tomcat/webapps
jar : tomcat/lib

ALTERNATIVES: NGINX, IIS, WEBSPHERE, JBOSS, GLASSFISH


SETUP: CREATE A NEW SERVER
INSTALL JAVA: amazon-linux-extras install java-openjdk11 -y

STEP-1: DOWNLOAD TOMCAT (dlcdn.apache.org)
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.93/bin/apache-tomcat-9.0.93.tar.gz

STEP-2: EXTRACT THE FILES
tar -zxvf apache-tomcat-9.0.93.tar.gz

STEP-3: CONFIGURE USER, PASSWORD & ROLES
vim apache-tomcat-9.0.93/conf/tomcat-users.xml

 56   <role rolename="manager-gui"/>
 57   <role rolename="manager-script"/>
 58   <user username="tomcat" password="raham123" roles="manager-gui, manager-script"/>

STEP-4: DELETE LINE 21 AND 22
vim apache-tomcat-9.0.93/webapps/manager/META-INF/context.xml

STEP-5: STARTING TOMCAT
sh apache-tomcat-9.0.91/bin/startup.sh

CONNECTION:
COPY PUBLIC IP:8080 
manager apps -- > username: tomcat & password: raham123

==============================================================

RBAC:

RBAC: ROLE BASE ACCESS CONTROL.
TO restrict the user PERMISSIONS in jenkins.

suresh	= fresher
raham	= exp 

STEP-1: USER CREATION
manage jenkins -- > users -- > create users -- > suresh: fresher 

STEP-2: PLUGIN DOWNLOADING
Dashboard
Manage Jenkins
Plugins
Available plugin
Role-based Authorization Strategy  

STEP-3: CONFIGURE THE PLUGIN
Dashboard
Manage Jenkins
Security
Authorization 
Role-based  Strategy  
SAVE

STEP-4: MANAGE AND ASSIGN USERS
manage roles -- > add -- > fresher & exp -- > fresher: overall read & exp: admin -- > save
assign roles -- > add user -- > rajesh: fresher -- > save



LINKED JOBS:
ONE JOB IS LINKED WITH ANOTHER JOB
IF WE BUILD JOB-1 AUTOMATICALLY JOB-2 IS GOING TO BUILD.

JOB-1 --> CONFIGURE -- > POSTBILD ACTIONS -- > BILD OTHER PROJECTS -- > JOB-2 -- > SAVE
BUILD JOB-1 AND JOB-2 WILL BE TRIGGERED


ASSIGNMENT:

1. CREATE 4 SERVERS (1=JENKINS, 1=NEXUS, 2=SLAVES)
2. CREATE 3 JOBS 
JOB-1: CI JOB -- > FREE STYLE		  (JENKINS) = abcd
JOB-2: PRINT THE ENV VARS -- > FREE STYLE (SLAVE-1) = /tmp
JOB-3: CI JOB WITH NEXUS -- > PIPELINE    (SLAVE-2) = /tmp

3. IF WE TRINGGER JOB-1 IT NEED TO TRIGGER JOB-2 & JOB-3
4. JOB-1 NEED TO RUN AUTOMATICALLY.


=========================================

PROJECT:

STEPS:
1. CREATE JENKINS SERVER 
2. CREATE 2 SLAVES AND CONFIGURE THEM TO JENKINS
3. INSTALL TOMCAT ON SLAVE -- > connect to tomcat
4. ADD CREDENTIALS OF SLAVE TO JENKINS
5. CREATE NEXUS SERVER
6. DOWNLOAD PLUGIN  Nexus Artifact Uploader
7. CONFIGURE NEXUS ON JENKINS
5. INSTALL DEPLOY TO CONTAINER PLUGIN
6. WRITE AND RUN THE PIPELINE 

GIT 
GITHUB
MAVEN
JENKINS
TOMCAT
NEXUS
SONARQUBE

pipeline {
    agent {
        label 'slave1'
    }
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('Artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('nexus upload') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: 'war']], credentialsId: 'd5f69729-fd4a-4ba2-baa9-91d20e34aeb1', groupId: 'in.RAHAM', nexusUrl: '18.206.123.44:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'netflix', version: '1.2.2'
            }
        }
        stage('Deploy') {
            steps {
                deploy adapters: [
                  tomcat9(
                    credentialsId: 'cf89c0c9-2fe8-4ae9-91f6-44af3bf240ab',
                    path: '',
                    url: 'http://54.208.15.180:8080/'
                    )    
                ],
                contextPath: 'netflix',
                war: 'target/*.war'
            }
        }
    }
}

=================================================================


Automated: Deployment, Installation
Non-Automated: Server
To perform end-to-end automation we can use Ansible.
Creating servers
configure servers
deployment application on servers

ANSIBLE:
its a Configuration Management Tool.
Configuration: Hardware and Software 
Management: Pkgs update, installing, remove ----
Ansible is used to manage and work with multiple servers together.
its a free and Opensource.
it is used to automate the entire deployment process on multiple servers.
We install Python on Ansible.
we use a key-value format for the playbooks.

PLAYBOOK:
create servers
install packages & software
deploy apps
---------

Jenkins = pipeline = groovy
ansible = playbooks = yaml


HISTORY:
in 2012 dev called Maichel Dehaan who developed ansible.
After few years RedHat taken the ansible.
it is platform-independent & will work on all linux flavours.


ARCHITECTURE:
PLAYBOOK: its a file which consist of code
INVENTORY: its a file which consist ip of nodes
SSH: used to connect with nodes
Ansible is Agent less.
Means no need to install any software on worker nodes.

SETUP: 
CREATE 5 SERVERS [1=ANSIBLE, 2=DEV, 2=TEST]

EXECUTE THE BELOW COMMANDS ON ALL SERVERS:
sudo -i
hostnamectl set-hostname ansible/dev-1/dev-2/test-1/test-2
sudo -i

passwd root  -- > to login to other servers
vim /etc/ssh/sshd_config (38 & 61 uncomment both lines) 
systemctl restart sshd
systemctl status sshd
hostname -i

THE BELOW STEPS NEED TO BE RUN ON ANSIBLE SERVER:

amazon-linux-extras install ansible2 -y
yum install python3 python-pip python-dlevel -y

vim /etc/ansible/hosts
# Ex 1: Ungrouped hosts, specify before any group headers.
[dev]
172.31.20.40
172.31.21.25
[test]
172.31.31.77
172.31.22.114

ssh-keygen -- > enter 4 times 
ssh-copy-id root@private ip of dev-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of dev-2 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-2 -- > yes -- > password -- > ssh private ip -- > ctrl d

ansible -m ping all : To check worker node connection with ansible server.

1. ADHOC COMMANDS:
these are simple Linux commands. 
these are used for temp works.
these commands will be over ridden.

ansible all -a "yum install git -y"
ansible all -a "yum install maven -y"
ansible all -a "mvn --version"
ansible all -a "touch file1"
ansible all -a "touch raham.txt"
ansible all -a "ls"
ansible all -a "yum install httpd -y"
ansible all -a "systemctl status httpd"
ansible all -a "systemctl start httpd"
ansible all -a "user add raham"
ansible all -a "cat /etc/passwd"
ansible all -a "yum remove git* maven* httpd* -y"


2. MODULES:
its a key-value pair.
modules are reusable.
we can use different modules for different purposes.
module flag is -m 

ansible all -m yum -a "name=git state=present"
ansible all -m yum -a "name=maven state=present"
ansible all -m yum -a "name=maven state=present"	[present=installed]
ansible all -m service -a "name=httpd state=started"	[started=restart]
ansible all -m service -a "name=httpd state=stopped"	[stopped=stop]
ansible all -m yum -a "name=http state=absent"		[absent=uninstall]
ansible all -m user -a "name=vikram state=present"
ansible all -m user -a "name=vikram state=absent"
ansible all -m copy -a "src=raham.txt dest=/tmp"

3. PLAYBOOKS:
playbooks used to execute multiple modules.
we can reuse the playbook multiple times.
in real time we use a playbook to automate our work.
for deployment, pkg installation, Server Creation ----
here we use key-value pairs.
Key-Value can also be called as Dictionary.
ansible-playbook will be written on YAML syntax.
YAML = YET ANOTHER MARKUP LANGUAGE
extension for playbook is .yml or .yaml
playbook start with --- and end with ... (opt)


EX-1:

- hosts: all
  tasks:
    - name: installing git
      yum: name=git state=present

    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: create user
      user: name=jayanth state=present

    - name: copy a file
      copy: src=index.html dest=/root

    

TO EXECUTE: ansible-playbook playbok.yml

Gather facts: it will get information of worker nodes
its by default task performed by ansible.

ok=total number of tasks
changed= no.of tasks successfully executed

EX-2:
 hosts: all
  ignore_errors: true
  tasks:
    - name: installing git
      yum: name=git state=absent

    - name: installing httpd
      yum: name=httpd state=absent

    - name: starting httpd
      service: name=httpd state=started

    - name: create users
      user: name=pushpa state=absent

    - name: copying a file
      copy: src=raham.txt dest=/root


TAGS: by default ansible will execute all tasks sequentially in a playbook.
we can use tags to execute a specific tasks or to skip a specific tasks.


EX-1:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: installing git
      yum: name=git state=present
      tags: a

    - name: installing httpd
      yum: name=httpd state=present
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: create a user
      user: name=kohli state=present
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SINGLE TAG: ansible-playbook raham.yml --tags d
MULTI TAGS: ansible-playbook raham.yml --tags b,c

EX-2:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: uninstalling git
      yum: name=git* state=absent
      tags: a

    - name: uninstalling httpd
      yum: name=httpd state=absent
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: delete a user
      user: name=kohli state=absent
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SKIP A SINGLE TASK: ansible-playbook raham.yml --skip-tags "c"
SKIP MULTIPLE TASK: ansible-playbook raham.yml --skip-tags "a,c"


==========================================================

HANDLERS:
when we have two tasks in a single playbook if task 2 is depending upon task 1 so then we can use the concept called handlers .
once task one is executed successfully it will notify task 2 to perform the operation. 
the name of the notify and the name of the task two must be same.


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

sed -i 's/present/absent/g' raham.yml

- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=absent
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

SETUP MODULE: used to print the complete info of worker nodes
ansible all -m setup 

ansible all -m setup  | grep -i family
ansible all -m setup  | grep -i pkg
ansible all -m setup  | grep -i cpus
ansible all -m setup  | grep -i mem


CONDITIONS:
CLUSTER: Group of servers
HOMOGENIUS: all servers have having same OS and flavour.
HETROGENIUS: all servers have different OS and flavour.

used to execute this module when we have different Clusters.

RedHat=yum
Ubuntu=apt

- hosts: all
  tasks:
    - name: installing git on RedHat
      yum: name=git state=present
      when: ansible_os_family == "RedHat"

    - name: installing git on Debian
      apt: name=git state=present
      when: ansible_os_family == "Debian"


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      when: ansible_nodename == "dev-1"

    - name: installing mysql
      yum: name=mysql state=present
      when: ansible_nodename == "dev-2"

    - name: installing python
      yum: name=python state=present


VALIDATORS:
1. YAMLINT
2. YAML FORMATTER
3. YAML VALIDATOR
4. CHATGPT

SHELL VS COMMAND VS RAW:

- hosts: all
  tasks:
    - name: installing maven
      shell: yum install maven -y

    - name: installing httpd
      command: yum install httpd -y

    - name: installing docker
      raw: yum install docker -y

raw >> command >> shell.

ansible all -a "mvn -v"
ansible all -a "htppd -v"
ansible all -a "docker -v"


L	: LINUX
A	: APACHE
M	: MYSQL
P	: PYTHON


- hosts: all
  tasks:
    - name: installing apache
      yum: name=httpd state=present

    - name: installing mysql
      yum: name=mysql state=present

    - name: installing python
      yum: name=python3 state=present


 ansible all -a "httpd --version"
 ansible all -a "python3 --version"
 ansible all -a "mysql --version"


=============================================


SHORTCUTS FOR LINUX:
vim .bashrc
alias ar='ansible-playbook raham.yml'
alias info='ansible all -m setup'

source .bashrc

VARIABLES:

STATIC VARS: we can define these vars inside the playbook and use for multiple times, once a variable is defined here it will not change untill we change.


- hosts: all
  vars:
    a: maven
    b: httpd
  tasks:
    - name: installing maven
      yum: name={{a}} state=present
    - name: installing httpd
      yum: name={{b}} state=present

TO EXECUTE: ansible-playbook playbbok.yml

DYNAMIC VARS: therse vars will be defined outside the playbook and these will change as per our requirments.

- hosts: all
  vars:
  tasks:
    - name: installing maven
      yum: name={{a}} state=absent
    - name: installing httpd
      yum: name={{b}} state=absent


ansible-playbook raham.yml --extra-vars "a=docker b=httpd"


LOOPS: We can use loops to reduce the length of the code for the playbook

- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{item}} state=present
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd


ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"


- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{item}} state=absent
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd

ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"

EX-2:

- hosts: all
  tasks:
    - name: creating users
      user: name={{item}} state=present
      with_items:
        - ravi
        - shiva
        - rajesh
        - shivani
        - luckyy


- hosts: all
  tasks:
    - name: creating users
      user: name={{item}} state=absent
      with_items:
        - ravi
        - shiva
        - rajesh
        - shivani
        - lucky


DEBUG: to print the messages from a playbook.

- hosts: all
  tasks:
    - name: printing a msg
      debug:
        msg: hai all welcome to my session

ansible all -m setup

NAME	: ansible_nodename
FAMILY  : ansible_os_family
PKG	: ansible_pkg_mgr
CPU	: ansible_processor_cores
MEM	: ansible_memtotal_mb
FREE	: ansible_memfree_mb


- hosts: all
  tasks:
    - name: print a msg
      debug:
        msg: "my node name is: {{ansible_nodename}}, the os is: {{ansible_os_family}}, the package manager is: {{ansible_pkg_mgr}}, total cpus is: {{ansible_processor_cores}}, the total ram: {{ansible_memtotal_mb}}, free ram is: {{ansible_memfree_mb}}"


JINJA2 TEMPLATE: used to get the customized op, here its a text file which can extract the variables and these values will change as per time.

LOOKUPS: this module used to get data from files, db and key values

- hosts: dev
  vars:
    a: "{{lookup('file', '/root/creds.txt') }}"
  tasks:
    - debug:
        msg: "hai my user name is {{a}}"

cat creds.txt
user=raham


STRATAGIES: Way of executing the playbook.

LINEAR: execute tasks sequentially 
if task-1 is executed on server-1 it will wait till task-2 execution
FREE: execute all tasks on all node at same time
if task-1 is executed on server-1 it wont wait till task-2 execution
ROLLING:
BATCH:

========================================================================


 roles
     pkgs
        tasks
            main.yml
     users
        tasks
            main.yml
     webserver
         tasks
             main.yml


ROLES:
roles is a way of organizing playbooks in a structured format.
main purpose of roles is to encapsulate the data.
we can reuse the roles multiple times.
length of the playbook is decreased.
it contains on vars, templates, task -----
in real time we use roles for our daily activities.
yum install tree -y

mkdir playbooks
cd playbooks/

mkdir -p roles/pkgs/tasks
vim roles/pkgs/tasks/main.yml

- name: installing pkgs
  yum: name=git state=present
- name: install maven
  yum: name=maven state=present
- name: installing docker
  yum: name=docker state=present

mkdir -p roles/users/tasks
vim roles/users/tasks/main.yml

- name: create users
  user: name={{item}} state=present
  with_items:
    - uday
    - naveen
    - rohit
    - lokesh
    - saipallavi
    - supriya

mkdir -p roles/webserver/tasks
vim roles/web/tasks/main.yml

- name: installing httpd
  yum: name=httpd state=present

- name: starting httpd
  service: name=httpd state=started

cat master.yml

- hosts: all
  roles:
    - pkgs
    - users
    - webserver

find . -type f -exec sed -i 's/present/absent/g' {} \;


ANSIBLE GALAXY:

Ansible Galaxy is a  website where users can share roles and to a command-line tool for installing, creating, and managing roles.
Ansible Galaxy gives greater visibility to one of Ansible's most exciting features, such as application installation or reusable roles for server configuration. 
Lots of people share roles in the Ansible Galaxy.
Ansible roles consist of many playbooks, which is a way to group multiple tasks into one container to do the automation in a very effective manner with clean, directory structures.

ANSIBLE VAULT:
Note: we can restrict the users to access the playbook also.
it is used to encrypt the files, playbooks ----
Technique: AES256 (USED BY FACEBOOK, AWS)
vault will store our data very safely and securely.
if we want to access any data which is in the vault we need to give a password.

cat creds.txt
user=raham
passowrd=test123

ansible-vault create creds1.txt		: to create a vault
ansible-vault edit creds1.txt		: to edit a vault
ansible-vault rekey creds1.txt		: to change password for a vault
ansible-vault decrypt creds1.txt	: to decrypt the content	
ansible-vault encrypt creds1.txt	: to encrypt the content	
ansible-vault view creds1.txt		: to show the content without decrypt

PIP: its a pkg manager used to install python libs/modules

Redhat: yum
ubuntu: apt
python: pip

- hosts: all
  tasks:
    - name: install pip
      yum: name=pip state=present

    - name: installing NumPy
      pip: name=NumPy state=present

    - name: installing Pandas
      pip: name=Pandas state=present

ASYNCHRONOUS & POLLING ACTIONS:
for every task in  ansible we can set time limit
if the task is not performed in that time limit ansible will stop playbook execution
this is called as asynchronous and polling.

- hosts: all
  ignore_errors: yes
  tasks:
    - name: sleeping
      command: sleep 30
      async: 20
      poll: 10
    - name: install git
      yum: name=git state=present


async: time we set for task to complete
poll: it will check if task is completed or not for every 10 sec

WEB SERVER : TO SHOW THE APP : httpd  : 80  : /var/www/html
frontend code
APP SERVER : TO USE THE APP : Tomcat  : 8080  : tomcat/webapps
frontend code + backend code


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: installing git
      yum: name=git state=present

    - name: checkout
      git:
        repo: https://github.com/RAHAMSHAIK007/netflix-clone.git
        dest: /var/www/html
      tags: a

==============================================================================


LINK FOR SCRIPTS & PLAYBOOKS : https://github.com/RAHAMSHAIK007/all-setups.git
LINK FOR PROJECT: https://github.com/devopsbyraham/jenkins-java-project.git

Install Jenkins on ansible server and connect to dashboard

SETP-1: PUSHING THE CODE FROM GIT TO GITHUB
STEP-2: PERFORM CI ON JENKINS
STEP-3: STORE ARTIFACT ON NEXUS
STRP-4: INSTALL TOMCAT ON NODES
SETP-5: COPY WAR FILE TO TOMCAT

INTEGRTAING ANSIBE WITH JENKINS:

1. install ansible plugin
2. configure ansible tool 
manage jenkins -- > tools -- > ansible -- > name: ansible & Path to ansible executables directory: /usr/bin -- > save (NOTE: /usr/bin is a folder where all Linux commands will store)

3. SAMPLE STEP: ANSIBLE PLAYBOOK
Ansible tool: ansible -- > 
Playbook file path in workspace: /etc/ansible/playbook.yml -- > 
Inventory file path in workspace: /etc/ansible/hosts -- > 
SSH CREDS: give creds of ansible & worker nodes -- > 
Disable the host SSH key check -- > 
generate script

18.215.171.109:8080/

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('nexus upload') {
            steps {
                echo "artifact is uploaded to nexus"
            }
        }
        stage('deploy') {
            steps {
                ansiblePlaybook credentialsId: '959fbd45-dd0d-44d9-95e6-dc2c41c7c58e', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', playbook: '/etc/ansible/playbook.yml', vaultTmpPath: ''
            }
        }
    }
}


cat playbook.yml
- hosts: all
  tasks:

    - name: task1
      copy:
        src: /var/lib/jenkins/workspace/pipeline/target/NETFLIX-1.2.2.war
        dest: /root/tomcat/webapps

================================================================================================================
CREATING EC2 FROM PLAYBOOK:

NOTE: If Ansible want to create an Ec2 instance in the cloud it need to have permission 
so to allocate the permission for our Ansible we can create IAM user

IAM -- > create user -- > name: Ansible -- > next -- > Attach policies directly -- > ec2fullAccess -- > next -- > create user 

Ansible -- > Security Credentials -- > Create access key -- > CLI -- > checkbox -- > create access key --> download .csv file

COME TO ANSIBLE SERVER:
aws configure -- > giving ansible user permissions to server

AWS Access Key ID [None]: xxxxxxxxxxxxx
AWS Secret Access Key [None]: xxxxxxxxxxxx
Default region name [None]: us-east-1
Default output format [None]: table

sudo pip install boto

- hosts: localhost
  tasks:
    - name: creating ec2 instance
      ec2:
        region: "us-east-1"
        count: 3
        image: "ami-04ff98ccbfa41c9ad"
        instance_type: "t2.micro"
        instance_tags:
          Name: "abc"


HOW TO ADD PARAMETERS:

PARAMETERS: Used to pass information for jobs


CHOICE: to pass single input at a time.
STRING: to pass multiple inputs at a time.
MULTI-LINE STRING: to pass multiple inputs on multiple lines at a time.
FILE: to pass the file as input.
BOOL: to pass input either yes or no.


This project is parameterized
Name: server 
Choices: 
dev
test
save


SONARQUBE:

SONARQUBE:
it is used to check the quality of code.
it supports 20+ programming languages.

ADV:
it will make bugs finding easy.
it will improve the dev skills.
code duplication will be avioded.
vulunerabilities can be assesed.
code smells.


port: 9000
dependency: java11
req: t2.medium


install SonarQube from below script:
https://github.com/RAHAMSHAIK007/all-setups.git

port: 9000

generate a token
add project -- > manual -- > name -- > token -- >  066d13acedb378668cfb4a343e45478dacb9b7ae

1. download plugin and restart Jenkins
2. configure tool 
dashboard -- > manage Jenkins -- > system -- >SonarQube -- > name: sonarube & url: ----- & add secret text.
create a project in SonarQube 
generate a token and give to Jenkins. 
3. configure maven tool
dashboard -- > tools -- > maven -- > name: maven -- > save


CODE:

node {
    stage('checkout') {
        git 'https://github.com/devopsbyraham/jenkins-java-project.git'
    }
    stage('build') {
        sh 'mvn compile'
    }
    stage('test') {
        sh 'mvn test'
    }
    stage('artifact') {
        sh 'mvn package'
    }
    stage("code quality") {
     withSonarQubeEnv('sonarqube') 
    {
      def mavenHome = tool name: "maven", type: "maven"
      def mavenCMD = "${mavenHome}/bin/mvn"
      sh "${mavenCMD} sonar:sonar"
    }
  }

}

================================================================================

APPLICATION: Collection of services 

MONOLITHIC: multiple services are deployed on single server with single database.
MICRO SERVICES: multiple services are deployed on multiple servers with multiple database.

BASED ON USERS AND APP COMPLEXITY WE NEED TO SELECT THE ARCHITECTURE.

FACTORS AFFECTING FOR USING MICROSERVICES:
F-1: COST 
F-2: MAINTAINANCE

CONTAINERS:
its free of cost and can create multiple containers.
its same as a server/vm.
it will not have any operating system.
os will be on images.
(SERVER=AMI, CONTAINER=IMAGE)

DOCKER: 
Its an free & opensource tool.
it is platform independent.
used to create, run & deploy applications on containers.
it is introduced on 2013 by solomenhykes & sebastian phal.
We used GO language to develop the docker.
here we write files on YAML.
before docker user faced lot of problems, but after docker there is no issues with the application.
Docker will use host resources (cpu, mem, n/w, os).
Docker can run on any OS but it natively supports Linux distributions.

CONTAINERIZATION/DOCKERIZATION:
Process of packing an application with its dependencies.
ex: PUBG

APP= PUBG & DEPENDECY = MAPS
APP= CAKE & DEPENDECY = KNIFE

os level of virtualization.

VIRTUALIZATION:
able to create resource with our hardware properties.

ARCHITECTURE & COMPONENTS:
client: it will interact with user
user gives commands and it will be executed by docker client

daemon: manages the docker components(images, containers, volumes)

host: where we install docker (ex: linux, windows, macos)

Registry: manages the images.

ARCHITECTURE OF DOCKER:
yum install docker -y    #client
systemctl start docker	 #client,Engine
systemctl status docker


COMMANDS:
docker pull ubuntu	: pull ubuntu image
docker images		: to see list of images
docker run -it --name cont1 ubuntu : to create a container
-it (interactive) - to go inside a container
cat /etc/os-release	: to see os flavour


apt update -y	: to update 
redhat=yum
ubuntu=apt
without update we cant install any pkg in ubuntu


apt install git -y
apt install apache2 -y
service apache2 start
service apache2 status

docker p q		: to exit container
docker ps -a		: to list all containers
docker attach cont_name	: to go inside container
docker stop cont_name	: to stop container
docker start cont_name	: to start container
docker pause cont_name	: to pause container
docker unpause cont_name: to unpause container
docker inspect cont_name: to get complete info of a container
docker rm cont_name	: to delete a container

STOP: will wait to finish all process running inside container
KILL: wont wait to finish all process running inside container

========================================================
OS LEVEL OF VIRTUALIZATION:
ability to take backup of complete os and reuse it.

docker pull ubuntu
docker run -it --name cont1 ubuntu
apt update -y
apt install mysql-server apache2 python3 -y
touch file{1...5}
apache2 -v
mysql-server --version
python3 --version
ls

ctrl p q

docker commit cont1 raham:v1
docker run -it --name cont2 raham:v1
apache2 -v
mysql-server --version
python3 --version
ls


DOCKERFILE:
it is an automation way to create image.
here we use components to create image.
in Dockerfile D must be Capiatl.
Components also capital.
This Dockerfile will be Reuseable.
here we can create image directly without container help.
Name: Dockerfile

docker kill $(docker ps -qa)
docker rm $(docker ps -qa)
docker rmi -f $(docker images -qa)

COMPONENTS:

FROM		: used to base image
RUN		: used to run linux commands (During image creation)
CMD		: used to run linux commands (After container creation)
ENTRYPOINT	: high priority than cmd
COPY		: to copy local files to conatiner
ADD		: to copy internet files to conatiner
WORKDIR		: to open req directory
LABEL		: to add labels for docker images
ENV		: to set env variables (inside container)
ARGS		: to pass env variables (outside containers)
EXPOSE		: to give port number


EX-1:
FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y

docker build -t raham:v1 .
docker run -it --name cont1 raham:v1 

EX-2:
FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
RUN apt install python3 -y
CMD apt install mysql-server -y


docker build -t raham:v2 .
docker run -it --name cont2 raham:v2

EX-3:
FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp

docker build -t raham:v3 .
docker run -it --name cont3 raham:v3

EX-4:

FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp
WORKDIR /tmp
LABEL author rahamshaik

docker build -t raham:v4 .
docker run -it --name cont4 raham:v4


EX-5:
FROM ubuntu
LABEL author rahamshaik
ENV client swiggy
ENV server appserver

docker build -t raham:v5 .
docker run -it --name cont5 raham:v5

INDEX.HTML LINK: https://www.w3schools.com/howto/tryit.asp?
filename=tryhow_css_form_icon

NETFLIX-DEPLOYMENT:

yum install git -y
git clone https://github.com/RAHAMSHAIK007/netflix-clone.git
mv netflix-clone/*

 Dockerfile

FROM ubuntu
RUN apt update
RUN apt install apache2 -y
COPY * /var/www/html/
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t netflix:v1 .
docker run -it --name netflix1 -p 80:80 netflix:v1

MULTI-STAGE BUILD:
if we build image-1 from docker file and use that image-1 to build other image.

Dockerfile  -- > image1

Dockerfile
FROM image1  -- > multi-stage build
-----
-------

ADV:
less time
less work
less complexity

========================================================

16-08-2024:
VOLUMES:
It is used to store data inside container.
volume is a simple directory inside container.
containers uses host resources (cpu, ram, rom).
single volume can be shared to multiple containers.
ex: cont-1 (vol1)  --- > cont2 (vol1) & cont3 (vol1) & cont4 (vol1)
at a time we can share single volume to single container only.
every volume will store under /var/lib/docker/volumes

METHOD-1:
DOCKER FILE:

FROM ubuntu
VOLUME ["/volume1"]

docker build -t raham:v1 .
docker run -it --name cont1 raham:v1
cd volume1/
touch file{1..5}
cat>file1
ctrl p q

docker run -it --name cont2 --volumes-from cont1 --privileged=true  ubuntu
cd /volume1
ll
touch file{6..10}
ctrl pq

docker attach cont1
cd volume1
ll

docker run -it --name cont3 --volumes-from cont1 --privileged=true  ubuntu

METHOD-2:
FROM CLI:

docker run -it --name cont4 -v volume2 ubuntu
cd volume2/
touch java{1..5}
ctrl p q

docker run -it --name cont5 --volumes-from cont4  ubuntu
cd volume2
ll
touch java{6..10}
ctrl p q
docker attach cont4
ls



METHOD-3: VOLUME MOUNTING

docker volume ls 		: to list volumes
docker volume create name	: to create volume
docker volume inspect volume3	: to get info of volume3
cd /var/lib/docker/volumes/volume3/_data 
touch python{1..5}
docker run -it --name cont5 --mount source=volume3,destination=/volue3 ubuntu
docker volume rm 	: to delete volumes
docker volume prune	: to delete unused volumes

HOST -- > CONTAINER:

cd /root
touch raham{1..5}
docker volume inspect volume4
cp * /var/lib/docker/volumes/volume4/_data
docker attach cont5 
ls /volume4


RESOURCE MANAGEMENT:
By default, docker containers will not have any limits for the resources like cpu ram and memory so we need to restrict resource use for container.

By default docker containers will use host resources(cpu, ram, rom)
Resource limits of docker container should not exceed the docker host limits.

docker stats  --> to check live cpu and memory

docker run -it --name cont7 --cpus="0.1" --memory="300mb" ubuntu
docker update cont7 --cpus="0.7" --memory="300mb"

JENKINS SETUP BY DOCKER:
docker run -it --name jenkins -p 8080:8080 jenkins/jenkins:lts

======================================================================
17-08-2024:
vim Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

Index.html: take form w3 schools 

docker build -t movies:v1 .
docker run -itd --name movies -p 81:80 movies:v1

docker build -t train:v1 .
docker run -itd --name train -p 82:80 train:v1

docker build -t dth:v1 .
docker run -itd --name dth -p 83:80 dth:v1

docker build -t recharge:v1 .
docker run -itd --name recharge -p 84:80 recharge:v1

docker ps -a -q		: to list container ids
docker kill $(docker ps -a -q) : to kill all containers 
docker rm $(docker ps -a -q) : to remove all containers 

Note: In the above process all the containers are managed and created one by one in real time we manage all the continers at same time so for that purpose we are going to use the concept called Docker compose.


DOCKER COMPOSE:
It's a tool used to manage multiple containers in single host.
we can create, start, stop, and delete all containers together.
we write container information in a file called a compose file.
compose file is in YAML format.
inside the compose file we can give images, ports, and volumes info of containers.
we need to download this tool and use it.

INSTALLATION:
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version


In Linux majorly you are having two type of commands first one is inbuilt commands which come with the operating system by default 
second one is download commands we are going to download with the help of yum, apt or Amazon Linux extras.
some commands we can download on binary files.

NOTE: linux will not give some commands, so to use them we need to download seperately
once a command is downloaded we need to move it to /usr/local/bin
because all the user-executed commands in linux will store in /usr/local/bin
executable permission need to execute the command



vim docker-compose.yml

version: '3.8'
services:
  movies:
    image: movies:v1
    ports:
      - "81:80"
  train:
    image: train:v1
    ports:
      - "82:80"
  dth:
    image: dth:v1
    ports:
      - "83:80"
  recharge:
    image: recharge:v1
    ports:
      - "84:80"

COMMANDS:
docker-compose up -d		: to create and start all containers
docker-compose stop		: to stop all containers
docker-compose start		: to start all containers
docker-compose kill		: to kill all containers
docker-compose rm		: to delete all containers
docker-compose down		: to stop and delete all containers
docker-compose pause		: to pause all containers
docker-compose unpause		: to unpause all containers
docker-compose ps -a		: to list the containers managed by compose file
docker-compose images		: to list the images managed by compose file
docker-compose logs		: to show logs of docker compose
docker-compose top		: to show the process of compose containers
docker-compose restart		: to restart all the compose containers
docker-compose scale train=10	: to scale the service


CHANGING THE DEFULT FILE:

by default the docker-compose wil support the following names
docker-compose.yml, docker-compose.yaml, compose.yml, compose.yaml

mv docker-compose.yml raham.yml
docker-compose up -d	: throws an error

docker-compose -f raham.yml up -d
docker-compose -f raham.yml ps
docker-compose -f raham.yml down


images we create on server.
these images will work on only this server.

git (local) -- > github (internet) = to access by others
image (local) -- > dockerhub (internet) = to access by others

Replace your username 

STEPS:
create dockerhub account
create a repo

docker tag movies:v1 vijaykumar444p/movies
docker login -- > username and password
docker push vijaykumar444p/movies


docker tag train:v1 vijaykumar444p/train
docker push vijaykumar444p/train


docker tag dth:v1 vijaykumar444p/dth
docker push vijaykumar444p/dth

docker tag recharge:v1 vijaykumar444p/recharge
docker push vijaykumar444p/recharge

docker rmi -f $(docker images -q)
docker pull vijaykumar444p/movies:latest

========================================================================

High Avaliabilty: more than one server
why: if one server got deleted then other server will gives the app

DOCKER SWARM:
its an orchestration tool for containers. 
used to manage multiple containers on multiple servers.
here we create a cluster (group of servers).
in that cluster, we can create same container on multiple servers.
here we have the manager node and worker node.
manager node will create & distribute the container to worker nodes.
worker node's main purpose is to maintain the container.
without docker engine we cant create the cluster.
Port: 2377
worker node will join on cluster by using a token.
manager node will give the token.

SETUP:
create 3 servers
install docker and start the service
hostnamectl set-hostname manager/worker-1/worker-2
Enable 2377 port 

docker swarm init (manager) -- > copy-paste the token to worker nodes
docker node ls

Note: individual containers are not going to replicate.
if we create a service then only containers will be distributed.

SERVICE: it's a way of exposing and managing multiple containers.
in service we can create copy of conatiners.
that container copies will be distributed to all the nodes.

service -- > containers -- > distributed to nodes


http://3.238.163.98:81/
http://3.94.187.195:81/
http://50.19.68.66:81/

docker service create --name movies --replicas 3 -p 81:80 vijaykumar444p/movies:latest
docker service ls		: to list services
docker service inspect movies	: to get complete info of service
docker service ps movies	: to list the containers of movies
docker service scale movies=10	: to scale in the containers
docker service scale movies=3	: to scale out the containers
docker service rollback movies	: to go previous state
docker service logs movies	: to see the logs
docker service rm movies	: to delete the services.

when scale in it follows lifo pattern.
LIFO MEANS LAST-IN FIRST-OUT.

Note: if we delete a container it will recreate automatically itself.
it is called as self healing.


CLUSTER ACTIVIES:
docker swarm leave (worker)	: to make node inactive from cluster
To activate the node copy the token.
docker node rm node-id (manager): to delete worker node which is on down state
docker node inspect node_id	: to get comple info of worker node
docker swarm join-token manager	: to generate the token to join

Note: we cant delete the node which is ready state
if we want to join the node to cluster again we need to paste the token on worker node


DOCKER NETWORKING:
Docker networks are used to make communication between the multiple containers that are running on same or different docker hosts. 

We have different types of docker networks.
Bridge Network		: SAME HOST
Overlay network		: DIFFERENT HOST
Host Network
None network

BRIDGE NETWORK: It is a default network that container will communicate with each other within the same host.

OVERLAY NETWORK: Used to communicate containers with each other across the multiple docker hosts.

HOST NETWORK: When you Want your container IP and ec2 instance IP same then you use host network

NONE NETWORK: When you dont Want The container to get exposed to the world, we use none network. It will not provide any network to our container.


To create a network: docker network create network_name
To see the list: docker network ls
To delete a network: docker network rm network_name
To inspect: docker network inspect network_name
To connect a container to the network: docker network connect network_name container_id/name
docker exec -it cont1  /bin/bash
apt update
apt install iputils-ping -y : command to install ping checks
ping ip-address of cont2
crtl pq

To disconnect from the container: docker network disconnect network_name container_name
To prune: docker network prune

DATABASE SETUP:
docker run -itd --name dbcont -e MYSQL_ROOT_PASSWORD=raham123 mysql:9.0.1
docker exec -it dbcont /bin/bash
mysql -u root -p


PROJECT REPO:
https://github.com/devopsbyraham/dockermsproject.git

==========================================================================

DOCKER ALTERNATIVES: containerd, rocket, cri-o


CHIRU   : CLUSTER
NAGABABU: NODE
PAWAN   : POD
CHARAN	: CONTAINER
ALLU	: APP

K8S:

LIMITATIONS OF DOCKER SWARM:
1. CANT DO AUTO-SCALING AUTOMATICALLY
2. CANT DO LOAD BALANCING AUTOMATICALLY
3. CANT HAVE DEFAULT DASHBOARD
4. WE CANT PLACE CONATINER ON REQUITED SERVER.
5. USED FOR EASY APPS. 

HISTORY:
Initially Google created an internal system called Borg (later called as omega) to manage its thousands of applications later they donated the borg system to cncf and they make it as open source. 
initial name is Borg but later cncf rename it to Kubernetes 
the word Kubernetes originated from Greek word called pilot or Hailsmen.
Borg: 2014
K8s first version came in 2015.


INTRO:

IT is an open-source container orchestration platform.
It is used to automates many of the manual processes like deploying, managing, and scaling containerized applications.
Kubernetes was developed by GOOGLE using GO Language.
MEM -- > GOOGLE -- > CLUSTER -- > MULTIPLE APPS OF GOOGLE -- > BORG -- > 
Google donated Borg to CNCF in 2014.
1st version was released in 2015.


ARCHITECTURE:

DOCKER : CNCA
K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION


COMPONENTS:
MASTER:

1. API SERVER: communicate with user (takes command execute & give op)
2. ETCD: database of cluster (stores complete info of a cluster ON KEY-VALUE pair)
3. SCHEDULER: select the worker node to shedule pods (depends on hw of node)
4. CONTROLLER: control the k8s objects (n/w, service, Node)

WORKER:

1. KUBELET : its an agent (it will inform all activites to master) It create containers.
2. KUBEPROXY: it deals with nlw (ip, networks, ports)
3. POD: group of conatiners (inside pod we have app)

Note: all components of a cluster will be created as a pod.


CLUSTER TYPES:

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)
kind:
k9s:
kubespray:


2. CLOUD-BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKS = GOOGLE KUBERENETS SERVICE



MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
Here Master and worker runs on same server.
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
It is a platform Independent.
Installing Minikube is simple compared to other tools.

NOTE: But we don't implement this in real-time Prod

REQUIREMENTS:

2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

Kubectl is the command line tool for k8s
if we want to execute commands we need to use kubectl.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

NOTE: When you download a command as binary file it need to be on /usr/local/bin 
because all the commands in linux will be on /usr/local/bin 
and need to give executable permission for that binary file to work as a  command.



POD:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE:

kubectl run pod1 --image vinodvanama/paytmmovies:latest
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DECRALATIVE: by using file called manifest file

MANDATORY FEILDS: without these feilds we cant create manifest

apiVersion:
kind:
metadata:
spec:


vim pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: vinodvanama/paytmtrain:latest
      name: cont1

execution: 
kubectl create -f pod.yml
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete -f raham.yml

DRAWBACK: once pod is deleted we can't retrieve the pod.

DOCKER ALTERNATIVES: containerd, rocket, cri-o


CHIRU   : CLUSTER
NAGABABU: NODE
PAWAN   : POD
CHARAN	: CONTAINER
ALLU	: APP

K8S:

LIMITATIONS OF DOCKER SWARM:
1. CANT DO AUTO-SCALING AUTOMATICALLY
2. CANT DO LOAD BALANCING AUTOMATICALLY
3. CANT HAVE DEFAULT DASHBOARD
4. WE CANT PLACE CONATINER ON REQUITED SERVER.
5. USED FOR EASY APPS. 

HISTORY:
Initially Google created an internal system called Borg (later called as omega) to manage its thousands of applications later they donated the borg system to cncf and they make it as open source. 
initial name is Borg but later cncf rename it to Kubernetes 
the word Kubernetes originated from Greek word called pilot or Hailsmen.
Borg: 2014
K8s first version came in 2015.


INTRO:

IT is an open-source container orchestration platform.
It is used to automates many of the manual processes like deploying, managing, and scaling containerized applications.
Kubernetes was developed by GOOGLE using GO Language.
MEM -- > GOOGLE -- > CLUSTER -- > MULTIPLE APPS OF GOOGLE -- > BORG -- > 
Google donated Borg to CNCF in 2014.
1st version was released in 2015.


ARCHITECTURE:

DOCKER : CNCA
K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION


COMPONENTS:
MASTER:

1. API SERVER: communicate with user (takes command execute & give op)
2. ETCD: database of cluster (stores complete info of a cluster ON KEY-VALUE pair)
3. SCHEDULER: select the worker node to shedule pods (depends on hw of node)
4. CONTROLLER: control the k8s objects (n/w, service, Node)

WORKER:

1. KUBELET : its an agent (it will inform all activites to master) It create containers.
2. KUBEPROXY: it deals with nlw (ip, networks, ports)
3. POD: group of conatiners (inside pod we have app)

Note: all components of a cluster will be created as a pod.


CLUSTER TYPES:

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)
kind:
k9s:
kubespray:


2. CLOUD-BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKS = GOOGLE KUBERENETS SERVICE



MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
Here Master and worker runs on same server.
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
It is a platform Independent.
Installing Minikube is simple compared to other tools.

NOTE: But we don't implement this in real-time Prod

REQUIREMENTS:

2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

Kubectl is the command line tool for k8s
if we want to execute commands we need to use kubectl.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

NOTE: When you download a command as binary file it need to be on /usr/local/bin 
because all the commands in linux will be on /usr/local/bin 
and need to give executable permission for that binary file to work as a  command.



POD:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE:

kubectl run pod1 --image vinodvanama/paytmmovies:latest
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DECRALATIVE: by using file called manifest file

MANDATORY FEILDS: without these feilds we cant create manifest

apiVersion:
kind:
metadata:
spec:


vim pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: vinodvanama/paytmtrain:latest
      name: cont1

execution: 
kubectl create -f pod.yml
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete -f raham.yml

DRAWBACK: once pod is deleted we can't retrieve the pod.


=================================================================================REPLICASET:
rs -- > pods
it will create multiple copies of same pod.
if we delete one pod automatically it will create new pod.
All the pods will have same config. (from Template)
only pod names will be differnet.


LABLES: individual pods are difficult to manage because they have different names.
so we give a common label to group them and work with them together
SELECTOR: Used to select pods with same labels.


use kubectl api-resources for checking the objects info

vim replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest




To list rs		:kubectl get rs/replicaset
To show addtional info	:kubectl get rs -o wide
To show complete info	:kubectl describe rs name-of-rs
To delete the rs	:kubectl delete rs name-of-rs
to get lables of pods 	: kubectl get pods -l app=Paytm
to delete pods		: kubectl delete po -l app=paytm
TO scale rs		: kubectl scale rs/movies --replicas=10 (LIFO)


LIFO: LAST IN FIRST OUT.
IF A POD IS CREATED LASTLY IT WILL DELETE FIRST WHEN SCALE OUT.

ADV:
Self healing
scaling

DRAWBACKS:
1. we cant Rollin and rollout, we cant update the application in rs.

DEPLOYMENT:
deploy -- > rs -- > pods
its high level k8s objects.
we can update the application.

vim deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest



To list deployment	:kubectl get deploy
To show addtional info	:kubectl get deploy -o wide
To show complete info	:kubectl describe deploy name-of-deployment
To delete the deploy	:kubectl delete deploy name-of-deploy
to get lables of pods 	:kubectl get pods -l app=paytm
TO scale deploy		:kubectl scale deploy/name-of-deploy --replicas=10 (LIFO)
To edit deploy		:kubectl edit deploy/name-of-deploy
to show all pod labels	:kubectl get pods --show-labels
To delete all pods	:kubectl delete pod --all


BY DEFUALT K8S WILL FOLLOW ROLLING-UPDATE STRATEGY.

kubectl rollout history deploy/movies
kubectl rollout undo deploy/movies
kubectl rollout status deploy/movies
kubectl rollout pause deploy/movies
kubectl rollout resume deploy/movies

COMMANDS FOR SHORTCUTS:

vim .bashrc

alias kgp="kubectl get pods"
alias kgr="kubectl get rs"
alias kgd="kubectl get deploy"

source .bashrc

kgr
kgp

================================================================================

KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, ASG-------------


Minikube -- > single node cluster
All the pods on single node 
if that node got deleted then all pods will be gone.

KOPS:
kOps, also known as Kubernetes operations.
it is an free and  open-source tool.
used to create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
AWS (Amazon Web Services) and GCE (Google Cloud Platform) are currently officially supported, with DigitalOcean, Hetzner and OpenStack in beta support, and Azure in alpha.

ADVANTAGES:
	Automates the provisioning of AWS and GCE Kubernetes clusters
	Deploys highly available Kubernetes masters
	Supports rolling cluster updates
	Autocompletion of commands in the command line
	Generates Terraform and CloudFormation configurations
	Manages cluster add-ons.
	Supports state-sync model for dry-runs and automatic idempotency
	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.



STEP-1: GIVING PERMISSIONS 

KOps Is a third party tool if it want to create infrastructure on aws 
aws need to give permission for it so we can use IAM user to allocate permission for the kops tool

IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure (run this command on server)

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATING BUCKET 
aws s3api create-bucket --bucket rahamsdevopsbatchmay29202479am.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket rahamsdevopsbatchmay29202479am.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://rahamsdevopsbatchmay29202479am.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster rahamdevops.k8s.local
 * edit your node instance group: kops edit ig --name=rahamdevops.k8s.local nodes-ap-south-1a
 * edit your master instance group: kops edit ig --name=rahamdevops.k8s.local master-ap-south-1a


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

ADMIN ACTIVITIES:
kops edit ig --name=rahams.k8s.local master-us-east-1a
kops update cluster --name rahamdevops.k8s.local --yes
kops rolling-update cluster

NOTE: In real time we use five node cluster two master nodes and three worker nodes.

NOTE: its My humble request for all of you not to delete the cluster manually and do not delete any server use the below command to delete the cluster.

TO DELETE: kops delete cluster --name rahamdevops.k8s.local --yes


==================================================================


NAMESPACES:

NAMESPACE: It is used to divide the cluster to multiple teams on real time.
it is used to isolate the env.

CLUSTER: HOUSE
NAMESPACES: ROOM
TEAM MATES: FAMILY MEM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
If dev team create a pod on dev ns testing team cant able to access it.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one node to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

NOTE: Every component of Kubernetes cluster is going to create in the form of pod
And all these pods are going to store on kUBE-SYSTEM ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces
kubectl get po --all-namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view : to see current namespace
kubectl run dev1 --image nginx
kubectl run dev2 --image nginx
kubectl run dev3 --image nginx
kubectl create ns test	: to create namespace
kubectl config set-context --current --namespace=test : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl get po -n dev
kubectl delete pod dev1 -n dev
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods

NOTE: BY DEFAULT K8S NAMESPACE WILL PROVIDE ISOLATION BUT NOT RESTRICTION.
TO RESTRICT THE USER TO ACCESS A NAMESPACE IN REAL TIME WE USE RBAC.
WE CREATE USER, WE GIVE ROLES AND ATTACH ROLE.

alias switch="kubectl config set-context --current"

SERVICE: It is used to expose the application in k8s.

TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: ClusterIP
  selector:
    app: movies
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)
if we dont sepcify k8s service will take random port number.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: NodePort
  selector:
    app: movies
  ports:
    - port: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC)
DRAWBACK:
EXPOSING PUBLIC-IP & PORT 
PORT RESTRICTION.

3. LOADBALACER: It will expose our app and distribute load blw pods.
it will expose the application with dns [Domain Name System] -- > 53
to crete dns we use Route53 

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
=====================================================================

DAEMONSET: used to create one pod on each workernode.
Its the old version of Deployment.
if we create a new node a pod will be automatically created.
if we delete a old node a pod will be automatically removed.
daemonsets will not be removed at any case in real time.
Usecases: we can create pods for Logging, Monitoring of nodes 


apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80


Replication controller is the previous version of replica set.

RC : uses equity based selector (=, =!)
RS: used set based selector ( env in (dev, test)

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: rahamshaik/moviespaytm:latest
        ports:
        - containerPort: 80






METRIC SERVER:
if we install metric server in k8s cluster it can collects metrics like cpu, ram -- from all the pods and nodes in cluster.
we can use kubectl top po/no to see metrics
previously we can called it as heapster.

Metrics Server offers:

    A single deployment that works on most clusters (see Requirements)
    Fast autoscaling, collecting metrics every 15 seconds.
    Resource efficiency, using 1 milli core of CPU and 2 MB of memory for each node in a cluster.
    Scalable support up to 5,000 node clusters.


You can use Metrics Server for:
CPU/Memory based horizontal autoscaling (Horizontal Autoscaling)
Automatically adjusting/suggesting resources needed by containers (Vertical Autoscaling)


Horizontal: New 
Vertical: Existing



In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or ReplicaSet), with the aim of automatically scaling the workload to match demand.

Example : if you have pod-1 with 50% load and pod2 with 50% load then average will be (50+50/2=50) average value is 50
but if pod-1 is exceeding 60% and pod-2 50% then average will be 55% (then here we need to create a pod-3 becaue its exceeding the average)

Here we need to use metric server whose work is to collect the metrics (cpu & mem info)
metrics server is connected to the HPA and give information to HPA 
Now HPA will analysis metrics for every 30 sec and create a new pod if needed.


COOLING PERIOD:


scaling can be done only for scalable objects (ex: RS, Deployment, RC )
HPA is implemented as a K8S API Resources and a controller.
Controller Periodically adjust the number of replicas in RS, RC and Deployment depends on average.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest


kubectl apply -f hpa.yml
kubectl get all
kubectl get deploy 
kubectl autoscale deployment movies --cpu-percent=20 --min=1 --max=10
kubectl get hpa
kubectl desribe hpa movies
kubectl get al1

open second termina and give
kubectl get po --watch

come to first terminal and go inside pod
kubectl exec mydeploy-6bd88977d5-7s6t8 -it -- /bin/bash

apt update -y
apt install stress -y
stress 

check terminal two to see live pods

======================================================================


QUOTAS:
k8s cluster can be divied into namespaces
By default the pod in K8s will run with no limitations of Memory and CPU
But we need to give the limit for the Pod 
It can limit the objects that can be created in a namespace and total amount of resources.
when we create a pod scheduler will the limits of node to deploy pod on it.
here we can set limits to CPU, Memory and Storage
here CPU is measured on cores and memory in bytes.
1 cpu = 1000 millicpus  ( half cpu = 500 millicpus (or) 0.5 cpu)

Here Request means how many we want
Limit means how many we can create maximum

limit can be given to pods as well as nodes
the default limit is 0

if you mention request and  limit then everything is fine
if you dont mention request and mention limit then Request=Limit
if you mention request and not mention limit then Request=!Limit

IMPORTANT:
Ever Pod in namespace must have CPU limts.
The amount of CPU used by all pods inside namespace must not exceed specified limit.

DEFAULT RANGE:
CPU : 
MIN		= REQUEST = 0.5
MAX		= LIMIT = 1

MEMORY :
MIN	= REQUEST = 500M
MAX	= LIMIT = 1G


kubectl create ns dev
kubectl config set-context $(kubectl config current-context) --namespace=dev

vim dev-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi

kubectl create -f dev-quota.yml
kubectl get quota


EX-1: Mentioning Limits  = SAFE WAY

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 512Mi

kubectl create -f dep.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "0.2"
              memory: 100Mi

kubectl create -f dep.yml



EX-2: MENTION LIMITS & REQUESTS = SAFE WAY


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: "0.2"
              memory: 100Mi

EX-3: MENTION only REQUESTS =  NOT SAFE WAY


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            requests:
              cpu: "0.2"
              memory: 100Mi


 =======================================================================

pv:

Persistent means always available.
PVs are independent they can exist even if no pod is using them.
it is created by administrator or dynamically created by a storage class. 
Once a PV is created, it can be bound to a Persistent Volume Claim (PVC), which is a request for storage by a pod.
When a pod requests storage via a PVC, K8S will search for a suitable PV to satisfy the request. 
PV is bound to the PVC and the pod can use the storage. 
If no suitable PV is found, K8S will either dynamically create a new one (if the storage class supports dynamic provisioning) or the PVC will remain unbound.

pvc:

To use Pv we need to claim the volume using PVC.
PVC request a PV with your desired specification (size, access, modes & speed etc) from k8s and onec a suitable PV is found it will bound to PVC
After bounding is done to pod you can mount it as a volume.
once userfinised its work the attached PV can be released the underlying PV can be reclaimed & recycled for future.

RESTRICTIONS:
1. Instances must be on same az as the ebs 
2. EBS supports only a sinlge EC2 instance mounting

pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-07e5c6c3fe273239f
    fsType: ext4

pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

dep.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
     app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: raham
        image: centos
        command: ["/bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: my-pv
          mountPath: "/tmp/persistent"
      volumes:
        - name: my-pv
          persistentVolumeClaim:
            claimName: my-pvc

kubectl exec pvdeploy-86c99cf54d-d8rj4 -it -- /bin/bash
cd /tmp/persistent/
ls
vim raham
exit

now delete the pod and new pod will created then in that pod you will see the same content.

MULTI CONTAINER PODS:

SIDE CAR:
It creates a helper container to main container.
main container will have application and helper container will do help for maiN container.

Adapter Design Pattern:
standardize the output pattern of main container.

Ambassador Design Pattern:
used to connect containers with the outside world

Init Conatiner:
it initialize the first work and exits later.

=============================================================================

PROMETHEUS:

Prometheus is an open-source monitoring system that is especially well-suited for cloud-native environments, like Kubernetes. 
It can monitor the performance of your applications and services.
it will sends an alert you if there are any issues. 
It has a powerful query language that allows you to analyze the data.
It pulls the real-time metrics, compresses and stores  in a time-series database.
Prometheus is a standalone system, but it can also be used in conjunction with other tools like Alertmanager to send alerts based on the data it collects.
it can be integration with tools like PagerDuty, Teams, Slack, Emails to send alerts to the appropriate on-call personnel.
it collects, and it also has a rich set of integrations with other tools and systems.
For example, you can use Prometheus to monitor the health of your Kubernetes cluster, and use its integration with Grafana to visualize the data it collects.

COMPONENTS OF PROMETHEUS:
Prometheus is a monitoring system that consists of the following components:

A main server that scrapes and stores time series data
A query language called PromQL is used to retrieve and analyze the data
A set of exporters that are used to collect metrics from various systems and applications
A set of alerting rules that can trigger notifications based on the data
An alert manager that handles the routing and suppression of alerts

GRAFANA:
Grafana is an open-source data visualization and monitoring platform that allows you to create dashboards to visualize your data and metrics. 
It is a popular choice for visualizing time series data, and it integrates with a wide range of data sources, including Prometheus, Elasticsearch, and InfluxDB.
A user-friendly interface that allows you to create and customize dashboards with panels that display your data in a variety of formats, including graphs, gauges, and tables. 
You can also use Grafana to set up alerts that trigger notifications when certain conditions are met.
Grafana has a rich ecosystem of plugins and integrations that extend its functionality. For example, you can use Grafana to integrate with other tools and services, such as Slack or PagerDuty, to receive alerts and notifications.

CONNECTION:
SETUP BOTH PROMETHEUS & GRAFAN FROM BELOW LINK
https://github.com/RAHAMSHAIK007/all-setups.git

pROMETHERUS: 9090
NODE EXPORTER: 9100
GRAFANA: 3000

CONNECTING PROMETHEUS TO GARAFANA:
connect to grafana dashboard -- > Data source -- > add -- > promethus -- > url of prometheus -- > save & test -- > top of page -- > explore data -- > if you want run some queries -- > top -- > import dashboard -- > 1860 -- > laod --- > prometheus -- > import 

amazon-linux-extras install epel -y
yum install stress -y


CONNECTING TO WORKER NODES:

Craete 2 servers and install node exporter
go to main server and 

vim /etc/hosts
public-ip node1  worker-1
public-ip node2  worker-2


SYNOPSIS:
PROMETEUS:
its a free & opensource monitoring tool
it collects metrics of nodes
it store metrics on time series database
we use PromQL language 
we can integrate promethus with tools like
pagerduty, slack and email to send notifications
PORT: 9090

GRAFANA:
its a visualization tool used to create dashboard.
Datasource is main component (from where you are getting data)
Prometheus will show data but cant create dashboards
Dashboards: create, Import  
we can integrate Grafana with tools like
pagerduty, slack and email to send notifications
PORT: 3000

username & password: admin & admin

NODE EXPORTER:
collects metrics of worker nodes
in each worker node we need to install node exporter
Port: 9100


10180
14731



HELM:

In K8S Helm is a package manager to install packages
in Redhat: yum & Ubuntu: apt & K8s: helm 

it is used to install applications on clusters.
we can install and deploy applications by using helm
it manages k8s resources packages through charts 
chart is a collection of files organized on a directory structure.
chart is collection of manifest files.
a running instance of a chart with a specific config is called a release.

INSTALLATION OF HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version


INSTALLATION OF METRIC SERVER:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml


INSTALL PROMETHEUS:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts

UPDATE HELM CHART REPOS:
helm repo update
helm repo list

CREATE PROMETHEUS NAMESPACE:
kubectl create namespace prometheus
kubectl get ns

INSTALL PROMETHEUS:
helm install prometheus prometheus-community/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2" --set server.persistentVolume.storageClass="gp2"
kubectl get pods -n prometheus
kubectl get all -n prometheus

CREATE GRAFANA NAMESPACE:
kubectl create namespace grafana

INSTALL GRAFANA:
helm install grafana grafana/grafana --namespace grafana --set persistence.storageClassName="gp2" --set persistence.enabled=true --set adminPassword='RahamDevOps' --set  service.type=LoadBalancer
kubectl get pods -n grafana
kubectl get service -n grafana

Copy the EXTERNAL-IP and paste in browser

Go to Grafana Dashboard  Add the Datasource  Select the Prometheus
add the below url in Connection and save and test
http://prometheus-server.prometheus.svc.cluster.local/


Import Grafana dashboard from Grafana Labs
grafana dashboard  new  Import  6417  load  select prometheus  import



NOW DEPLOY ANY APPLICATION AND SEE THE RESULT IN DASHBOARD.


ADD 315 PORT TO MONITOR THE FOLLOWING TERMS:
Network I/O pressure.
Cluster CPU usage.
Cluster Memory usage.
Cluster filesystem usage.
Pods CPU usage.

ADD 1860 PORT TO MONITOR NODES INDIVIDUALLY 

11454 -- > for pv and pvcs
747 -- > pod metrics
14623 -- > k8s overview db



SETTING ALREATS:

browser -- > my google account -- > security -- > two setp verification -- > App passwords -- > grafana -- > copy password 

sttx chyr zxmu bqfr 

vim /etc/grafana/grafana.ini
line 892

 892 [smtp]
 893 enabled = true
 894 host = smtp.gmail.com:587
 895 user = devopsbyraham@gmail.com
 896 # If the password contains # or ; you have to wrap it with triple quotes. Ex """#pa     ssword;"""
 897 password = xhfa drlb zgwm ogey
 898 ;cert_file =
 899 ;key_file =
 900 skip_verify = true
 901 from_address = devopsbyraham@gmail.com
 902 from_name = grafana

systemctl restart grafana-server.service
systemctl status grafana-server.service

Grafana -- > Aleart rules -- contact point -- > name -- > email -- > test -- > check gmail

aleating rules -- > new -- > set -- > name: Grafana -- > Metric: node_cpu_seconds_total -- > instace -- > localhost:9100 -- > 

Input: A & Function: Last & Mode: Strict 
Input: B & isabove: 0.7 (70%)

Folder: cpu
Evaluation group: cpu
Pending period: 30s 



ENV VARIABLES:

It is a way to pass configuration information to containers running within pods. 
To set Env   vars it include the env or envFrom field in the configuration file.

ENV: DIRECTLY PASSING
ENVFROM: PASSING FROM FILE

ENV:
allows you to set environment variables for a container, specifying a value directly for each variable that you name.

ENVFROM:
allows you to set environment variables for a container by referencing either a ConfigMap or a Secret. 
When you use envFrom, all the key-value pairs in the referenced ConfigMap or Secret are set as environment variables for the container. 
You can also specify a common prefix string

CONFIGMAPS:

It is used to store the data in key-value pair, files, or command-line arguments that can be used by pods, containers and other resources in cluster
But the data should be non confidential data ()
But it does not provider security and encryption.
If we want to provide encryption use secrets in kubernetes.
Limit of config map data in only 1 MB (we cannot store more than that)
But if we want to store a large amount of data in config maps we have to mount a volume or use a seperate database or file service.


USE CASES:
Configure application setting
Configuring a pod or container
Sharing configuration data across multiple resources
We can store the data: By using this config maps, we can store the data like IP address, URL's and DNS etc...

kubectl create deploy swiggydb --image=mariadb
kubectl get pods
kubectl logs swiggydb-5d49dc56-cbbqk

It is crashed why because we havent specified the password for it


kubectl set env deploy swiggydb MYSQL_ROOT_PASSWORD=Raham123 
kubectl get pods
now it will be on running state
kubectl delete deploy swiggydb

PASSING FROM VAR FILE:
kubectl create deploy swiggydb --image=mariadb
kubectl get pods
kubectl logs swiggydb-5d49dc56-cbbqk

vim vars

MYSQL_ROOT_PASSWORD=Raham123
MYSQL_USER=admin

kubectl create cm dbvars --from-env-file=varsfile
kubectl describe cm dbvars

kubectl get cm
kubectl set env deploy swiggydb --from=configmap/dbvars
kubectl get pods

SECRETS: To store sensitive data in an unencrypted format like passwords, ssh-keys etc ---
it uses base64 encoded format
password=raham (now we can encode and ecode the value)

WHY: if i dont want to expose the sensitive info so we use SECRETS
By default k8s will create some Secrets these are useful from me to create communicate inside the cluster
used to communicate with one resoure to another in cluster
These are system created secrets, we need not to delete

TYPES:
Generic: creates secrets from files, dir, literal (direct values)
TLS: Keys and certs
Docker Registry: used to get private images by using the password

kubectl create deploy swiggydb --image=mariadb
kubectl get po 
kubectl create secret generic password --from-literal=ROOT_PASSWORD=raham123 (from cli)
kubectl create secret generic my-secret --from-env-file=vars (from file)

kubectl get secrets
kubectl describe secret password
kubectl set env deploy swiggydb --from=secrets/password
kubectl get po 
kubectl set env deploy newdb --from=secret/password --prefix=MYSQL_

without passing prefix we cant make the pod running status

TO SEE SECRETS:
kubectl get secrets password -o yaml
echo -n "cmFoYW0xMjM" | base64 -d
echo -n "cmFoYW0xMjM" | base64 --decode

==================================================================================


ARGOCD:

INTRO:
	ArgoCD is a declarative continuous delivery tool for Kubernetes.
	ArgoCD is the core component of Argo Project.
	It helps to automate the deployment and management of applications in a K8s cluster. 
	It uses GitOps methodology to manage the application lifecycle and provides a simple and intuitive UI to monitor the application state, rollout changes, and rollbacks.
	With ArgoCD, you can define the desired state of your Kubernetes applications as YAML manifests and version control them in a Git repository. 
	ArgoCD will continuously monitor the Git repository for changes and automatically apply them to the Kubernetes cluster.
	ArgoCD also provides advanced features like application health monitoring, automated drift detection, and support for multiple environments such as production, staging, and development. 
	It is a popular tool among DevOps teams who want to streamline their Kubernetes application deployment process and ensure consistency and reliability in their infrastructure.



CREATE CLUSTER USING KOPS

INSTALL HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

INSTALL ARGO CD USING HELM
helm repo add argo-cd https://argoproj.github.io/argo-helm
helm repo update
kubectl create namespace argocd
helm install argocd argo-cd/argo-cd -n argocd
kubectl get all -n argocd



EXPOSE ARGOCD SERVER:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
yum install jq -y
export ARGOCD_SERVER='kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname''
echo $ARGOCD_SERVER
kubectl get svc argocd-server -n argocd -o json | jq --raw-output .status.loadBalancer.ingress[0].hostname
The above command will provide load balancer URL to access ARGO CD


TO GET ARGO CD PASSWORD:
export ARGO_PWD='kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d'
echo $ARGO_PWD
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
The above command to provide password to access argo cd

IN REAL FOR DEV &TEST ENV WE CAN USE 100% AUTOMATED DEPLOYENTS
FOR PRODUCTION WE PERFER SEMI-AUTOMATED.

CREATE APP AND FORCE IT
SETUP MONOTORING

INGRESS IN K8S:
Ingress helps to expose the HTTP and HTTPS routes from outside of the cluster.
Ingress supports 
Path-based  
Host-based routing
Ingress supports Load balancing and SSL termination.
It redirects the incoming requests to the right services based on the Web URL or path in the address.
Ingress provides the encryption feature and helps to balance the load of the applications.

Ingress is used to manage the external traffic to the services within the cluster which provides features like host-based routing, path-based routing, SSL termination, and more. Where a Load balancer is used to manage the traffic but the load balancer does not provide the fine-grained access control like Ingress.

Example:
Suppose you have multiple Kubernetes services running on your cluster and each service serves a different application such as example.com/app1 and example.com/app2. With the help of Ingress, you can achieve this. However, the Load Balancer routes the traffic based on the ports and can't handle the URL-based routing.

To install ingress, firstly we have to install nginx ingress controller:
command: kubectl create -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml

Once we install ingress controller, we have to deploy 2 applications. 
github url: https://github.com/mustafaprojectsindevops/kubernetes/tree/master/ingress
After executing all the files, use kubectl get ing to get ingress. After 30 seconds it will provide one load balancer dns.

access those applications using dns/nginx and dns/httpd. So the traffic will route into both the applications as per the routing
==========================================================================================================

